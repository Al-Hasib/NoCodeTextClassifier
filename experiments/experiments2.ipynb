{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from preprocessing import *\n",
    "from sklearn.svm import LinearSVC\n",
    "import joblib\n",
    "\n",
    "# Load train file\n",
    "train_path = Path(\"../ML Engineer/train.csv\")\n",
    "df = pd.read_csv(train_path)\n",
    "\n",
    "# convert the class attributes into number\n",
    "encoder = LabelEncoder()\n",
    "df['target'] = encoder.fit_transform(df['class'])\n",
    "\n",
    "# clean the training text\n",
    "currency_symbols = r'[\\$\\£\\€\\¥\\₹\\¢\\₽\\₩\\₪]'  \n",
    "text_cleaner = TextCleaner(currency_symbols)\n",
    "df['clean_text'] = df['email'].apply(lambda x: text_cleaner.clean_text(x))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn gensim nltk\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = pd.read_csv('movie_reviews.csv')\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['review'], data['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join([word for word in text if word not in string.punctuation])\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "X_train = X_train.apply(preprocess)\n",
    "X_test = X_test.apply(preprocess)\n",
    "\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [sentence.split() for sentence in X_train]\n",
    "w2v_model = Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def vectorize(sentence):\n",
    "    words = sentence.split()\n",
    "    words_vecs = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
    "    if len(words_vecs) == 0:\n",
    "        return np.zeros(100)\n",
    "    words_vecs = np.array(words_vecs)\n",
    "    return words_vecs.mean(axis=0)\n",
    "\n",
    "X_train = np.array([vectorize(sentence) for sentence in X_train])\n",
    "X_test = np.array([vectorize(sentence) for sentence in X_test])\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Precision:', precision_score(y_test, y_pred, pos_label='positive'))\n",
    "print('Recall:', recall_score(y_test, y_pred, pos_label='positive'))\n",
    "print('F1 score:', f1_score(y_test, y_pred, pos_label='positive'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('movie_reviews.csv')\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['review'], data['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the text data\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join([word for word in text if word not in string.punctuation])\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "X_train = X_train.apply(preprocess)\n",
    "X_test = X_test.apply(preprocess)\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = 100\n",
    "X_train = pad_sequences(X_train, maxlen=max_length, padding='post')\n",
    "X_test = pad_sequences(X_test, maxlen=max_length, padding='post')\n",
    "\n",
    "# Train the Word2Vec model\n",
    "sentences = [sentence.split() for sentence in X_train]\n",
    "w2v_model = Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)\n",
    "\n",
    "# Create a weight matrix for the embedding layer\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[i] = w2v_model.wv[word]\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Globe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "unzip glove.6B.zip\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "glove_file = 'glove.6B.100d.txt'  # Example for 100-dimensional embeddings\n",
    "embeddings_index = load_glove_embeddings(glove_file)\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Sample data\n",
    "sentences = [\n",
    "    \"Natural language processing makes computers understand text and speech.\",\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"Word embeddings capture semantic meanings of words.\",\n",
    "    \"Deep learning models are complex and require a lot of data.\"\n",
    "]\n",
    "labels = [0, 1, 0, 1]  # Sample binary labels\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Preprocess the sentences\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    return [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "\n",
    "processed_sentences = [preprocess_sentence(sentence) for sentence in sentences]\n",
    "\n",
    "def sentence_vector(sentence, embeddings_index, embedding_dim=100):\n",
    "    valid_words = [embeddings_index[word] for word in sentence if word in embeddings_index]\n",
    "    if not valid_words:\n",
    "        return np.zeros(embedding_dim)\n",
    "    return np.mean(valid_words, axis=0)\n",
    "\n",
    "# Convert sentences to vectors\n",
    "embedding_dim = 100  # Must match the dimension of GloVe embeddings used\n",
    "sentence_vectors = np.array([sentence_vector(sentence, embeddings_index, embedding_dim) for sentence in processed_sentences])\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(sentence_vectors, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a RandomForest classifier\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

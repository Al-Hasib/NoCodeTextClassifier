{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = Path(\"../ML Engineer/train.csv\")\n",
    "test_path = Path(\"../ML Engineer/test.csv\")\n",
    "submission_path = Path(\"../ML Engineer/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(train_path)\n",
    "test_data = pd.read_csv(test_path)\n",
    "submission_data = pd.read_csv(submission_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_</th>\n",
       "      <th>source</th>\n",
       "      <th>email</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5732aa7f-0c44-4a4f-877a-0488aed0d1f7</td>\n",
       "      <td>2</td>\n",
       "      <td>Subject: is the supply rebound beginning ? an ...</td>\n",
       "      <td>not_spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4d3c392d-a4f0-465d-baa3-2c15f1560f07</td>\n",
       "      <td>2</td>\n",
       "      <td>Subject: email list - 100 million addresses $ ...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d47e95c0-4909-41b8-aec8-a3fb953fa18f</td>\n",
       "      <td>4</td>\n",
       "      <td>Subject: alley dodecahedra suicide\\nare you re...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>658a83eb-689c-480a-ae31-d622dc83f9f8</td>\n",
       "      <td>6</td>\n",
       "      <td>Subject: ibuyit project\\ni wanted to share som...</td>\n",
       "      <td>not_spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>179d10b7-1c43-4e10-a0be-18d205b0fe24</td>\n",
       "      <td>4</td>\n",
       "      <td>Subject: cheap vicodin online - us fda pharmac...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    id_  source  \\\n",
       "0  5732aa7f-0c44-4a4f-877a-0488aed0d1f7       2   \n",
       "1  4d3c392d-a4f0-465d-baa3-2c15f1560f07       2   \n",
       "2  d47e95c0-4909-41b8-aec8-a3fb953fa18f       4   \n",
       "3  658a83eb-689c-480a-ae31-d622dc83f9f8       6   \n",
       "4  179d10b7-1c43-4e10-a0be-18d205b0fe24       4   \n",
       "\n",
       "                                               email     class  \n",
       "0  Subject: is the supply rebound beginning ? an ...  not_spam  \n",
       "1  Subject: email list - 100 million addresses $ ...      spam  \n",
       "2  Subject: alley dodecahedra suicide\\nare you re...      spam  \n",
       "3  Subject: ibuyit project\\ni wanted to share som...  not_spam  \n",
       "4  Subject: cheap vicodin online - us fda pharmac...      spam  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Subject: ibuyit project\\ni wanted to share some great news with you about a major corporate initiative\\nwithin ets , called the ibuyit project , that you should know about and be\\nlooking forward to . ibuyit is a tool that will make it easier for you to\\npurchase the things you need to get your job done and / or to operate our\\nbusiness . some key points related to ibuyit are :\\n? employees at all levels across ets will be able to access ibuyit on their\\ncomputers much like the enron intranet . ibuyit provides an upgrade to the\\nexisting b 2 b web - based procurement tool .\\n? ibuyit will utilize a new catalog system ( requisite ) that will make it\\neasier for you to identify material items and incorporate them into the\\nrequisition .\\n? ibuyit will not only make it simple for you to buy on - line , but it can also\\nserve as your single , convenient source to access other tools and information\\nyou use everyday at enron .\\nstan horton , rod hayslett , and phil lowry decided back in december that ets\\nwould take the lead by being the first business unit to begin working with\\nglobal strategic sourcing ( gss ) , who is developing ibuyit . both field and\\nhouston - based ets employees have been involved with ibuyit efforts to - date to\\nhelp determine how ibuyit can best be used to support our employees and our\\nbusiness . more of you will be involved as the ibuyit project progresses .\\nibuyit will be deployed to ets , corporate , ebs , ees , and ews in a phased\\napproach during the first three quarters of 2001 . starting in april , ibuyit\\nwill be available to certain groups of ets employees for the purchase of\\noffice supplies and travel . ibuyit will be implemented on a full scale\\nacross ets in july .\\nthose of you in engineering may be aware of the ongoing mlrp efforts and may\\nbe wondering about the impact of ibuyit . because of the unique nature of its\\nbusiness , engineering has decided to continue moving forward with mlrp . but ,\\nwe will also continue to explore ways for ibuyit to complement mlrp and\\nbenefit engineering \\x01 , s business .\\nas the person ultimately responsible for ibuyit \\x01 , s success within ets , i\\nwanted to share my enthusiasm with you as early as possible about ibuyit . in\\naddition to what i already mentioned , ibuyit strongly supports ets \\x01 , s existing\\nprocurement and supply management strategy , will allow the company to save\\nmoney by taking advantage of its relationships with preferred suppliers , and\\nwill help us to achieve a greater return on our investments in cutting - edge\\ntechnology . as you can tell , there are many strong reasons for all of us\\nwithin ets to be excited about ibuyit .\\nso , stay tuned . you may be asked to actively participate in ibuyit efforts ,\\nbut i guarantee that you will have many opportunities to hear and learn more\\nabout ibuyit in the coming months . you will also have many opportunities to\\nhave your questions answered and opinions heard - starting here . if you have\\nany questions or comments , simply reply to this e - mail - i will appreciate\\nhearing from you .\\nmorris brassfield\\nsenior director\\noperations support services\\n( 713 ) 646 - 7006'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['email'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train data shape: {train_data.shape}\\n\"\\\n",
    "      f\"Test data shape: {test_data.shape}\\n\"\\\n",
    "      f\"submission data shape: {submission_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.source.unique(), test_data.source.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming there's a 'label' column for classification\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='class', data=train_data, palette='coolwarm')\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data.email[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "train_data['email'] = train_data['email'].str.lower()\n",
    "train_data.dropna(subset=['email'], inplace=True)\n",
    "train_data['email'] = train_data['email'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "train_data['email'] = train_data['email'].str.strip()\n",
    "train_data['email'] = train_data['email'].str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "train_data.email[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    " \n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    doc = nlp(text)\n",
    "    # Remove stopwords\n",
    "    filtered_words = [token.text for token in doc if not token.is_stop]\n",
    "    # Join the filtered words to form a clean text\n",
    "    clean_text = ' '.join(filtered_words)\n",
    "    return clean_text\n",
    "\n",
    "remove_stopwords(train_data.email[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['email'] = train_data['email'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['text_length'] = train_data['email'].apply(len)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot text length distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(train_data['text_length'], bins=30, kde=True)\n",
    "plt.title('Distribution of Text Length')\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[train_data['text_length'] ==train_data['text_length'].min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[train_data['text_length'] ==train_data['text_length'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['text_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['text_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['text_length'] = test_data['email'].apply(len)\n",
    "print(test_data.text_length.min())\n",
    "test_data[test_data['text_length'] ==test_data['text_length'].min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Tokenize the cleaned text\n",
    "all_words = [word for tokens in train_data['email'].str.split() for word in tokens]\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "# Get the 20 most common words\n",
    "common_words = word_freq.most_common(20)\n",
    "\n",
    "# Plot the most common words\n",
    "words, counts = zip(*common_words)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=list(counts), y=list(words))\n",
    "plt.title('Top 20 Most Common Words')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['email'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(all_words))\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of Most Common Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = train_data[train_data['class']=='spam']\n",
    "not_spam = train_data[train_data['class']=='not_spam']\n",
    "\n",
    "spam_all_words = [word for tokens in spam['email'].str.split() for word in tokens]\n",
    "not_spam_all_words = [word for tokens in not_spam['email'].str.split() for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(spam_all_words))\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Most Common Words for Spam Mails')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(not_spam_all_words))\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Most Common Words for Non-Spam Mails ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfidfVectorizer\n",
    "Chi2, correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "N = 2\n",
    "for Product, category_id in sorted(category_to_id.items()):\n",
    "  features_chi2 = chi2(features, labels == category_id)\n",
    "  indices = np.argsort(features_chi2[0])\n",
    "  feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "  unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "  bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "  print(\"# '{}':\".format(Product))\n",
    "  print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-N:])))\n",
    "  print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-N:])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "models = [\n",
    "    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
    "    LinearSVC(),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state=0),\n",
    "]\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "for model in models:\n",
    "  model_name = model.__class__.__name__\n",
    "  accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n",
    "  for fold_idx, accuracy in enumerate(accuracies):\n",
    "    entries.append((model_name, fold_idx, accuracy))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
    "sns.stripplot(x='model_name', y='accuracy', data=cv_df, \n",
    "              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\n",
    "plt.show()\n",
    "\n",
    "cv_df.groupby('model_name').accuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC()\n",
    "\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, df.index, test_size=0.33, random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=category_id_df.Product.values, yticklabels=category_id_df.Product.values)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vast majority of the predictions end up on the diagonal (predicted label = actual label), where we want them to be. However, there are a number of misclassifications, and it might be interesting to see what those are caused by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "for predicted in category_id_df.category_id:\n",
    "  for actual in category_id_df.category_id:\n",
    "    if predicted != actual and conf_mat[actual, predicted] >= 10:\n",
    "      print(\"'{}' predicted as '{}' : {} examples.\".format(id_to_category[actual], id_to_category[predicted], conf_mat[actual, predicted]))\n",
    "      display(df.loc[indices_test[(y_test == actual) & (y_pred == predicted)]][['Product', 'Consumer_complaint_narrative']])\n",
    "      print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chi square test\n",
    "\n",
    "model.fit(features, labels)\n",
    "\n",
    "N = 2\n",
    "for Product, category_id in sorted(category_to_id.items()):\n",
    "  indices = np.argsort(model.coef_[category_id])\n",
    "  feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "  unigrams = [v for v in reversed(feature_names) if len(v.split(' ')) == 1][:N]\n",
    "  bigrams = [v for v in reversed(feature_names) if len(v.split(' ')) == 2][:N]\n",
    "  print(\"# '{}':\".format(Product))\n",
    "  print(\"  . Top unigrams:\\n       . {}\".format('\\n       . '.join(unigrams)))\n",
    "  print(\"  . Top bigrams:\\n       . {}\".format('\\n       . '.join(bigrams)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=df['Product'].unique()))\n",
    "\n",
    "# Create our list of punctuation marks\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Logistic Regression Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# Create pipeline using Bag of Words\n",
    "pipe = Pipeline([(\"cleaner\", predictors()),\n",
    "                 ('vectorizer', bow_vector),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "# model generation\n",
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stemming, lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "# Predicting with a test dataset\n",
    "predicted = pipe.predict(X_test)# Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))\n",
    "\n",
    "\n",
    "\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    " \n",
    "# Another sample text\n",
    "new_text = \"The majestic mountains provide a breathtaking view.\"\n",
    " \n",
    "# Remove stopwords using Gensim\n",
    "new_filtered_text = remove_stopwords(new_text)\n",
    " \n",
    "print(\"Original Text:\", new_text)\n",
    "print(\"Text after Stopword Removal:\", new_filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORD-COUNT\n",
    "df_train['word_count'] = df_train['text'].apply(lambda x: len(str(x).split()))\n",
    "print(df_train[df_train['target']==1]['word_count'].mean()) #Disaster tweets\n",
    "print(df_train[df_train['target']==0]['word_count'].mean()) #Non-Disaster tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for word embedding\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# PLOTTING WORD-COUNT\n",
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,4))\n",
    "train_words=df_train[df_train['target']==1]['word_count']\n",
    "ax1.hist(train_words,color='red')\n",
    "ax1.set_title('Disaster tweets')\n",
    "train_words=df_train[df_train['target']==0]['word_count']\n",
    "ax2.hist(train_words,color='green')\n",
    "ax2.set_title('Non-disaster tweets')\n",
    "fig.suptitle('Words per tweet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHARACTER-COUNT\n",
    "df_train['char_count'] = df_train['text'].apply(lambda x: len(str(x)))\n",
    "print(df_train[df_train['target']==1]['char_count'].mean()) #Disaster tweets\n",
    "print(df_train[df_train['target']==0]['char_count'].mean()) #Non-Disaster tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to lowercase, strip and remove punctuations\n",
    "def preprocess(text):\n",
    "    text = text.lower() \n",
    "    text=text.strip()  \n",
    "    text=re.compile('<.*?>').sub('', text) \n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  \n",
    "    text = re.sub('\\s+', ' ', text)  \n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text) \n",
    "    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d',' ',text) \n",
    "    text = re.sub(r'\\s+',' ',text) \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LEMMATIZATION\n",
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "from nltk.corpus import stopwords\n",
    "# This is a helper function to map NTLK position tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "# Tokenize the sentence\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
    "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
    "    return \" \".join(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalpreprocess(string):\n",
    "    return lemmatizer(stopword(preprocess(string)))\n",
    "df_train['clean_text'] = df_train['text'].apply(lambda x: finalpreprocess(x))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLITTING THE TRAINING DATASET INTO TRAIN AND TEST\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train[\"clean_text\"],df_train[\"target\"],test_size=0.2,shuffle=True)\n",
    "#Word2Vec\n",
    "# Word2Vec runs on tokenized sentences\n",
    "X_train_tok= [nltk.word_tokenize(i) for i in X_train]  \n",
    "X_test_tok= [nltk.word_tokenize(i) for i in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tf-Idf\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train) \n",
    "X_test_vectors_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "#building Word2Vec model\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "def fit(self, X, y):\n",
    "        return self\n",
    "def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0)) df['clean_text_tok']=[nltk.word_tokenize(i) for i in df['clean_text']]\n",
    "model = Word2Vec(df['clean_text_tok'],min_count=1)     \n",
    "modelw = MeanEmbeddingVectorizer(w2v)\n",
    "# converting text to numerical data using Word2Vec\n",
    "X_train_vectors_w2v = modelw.transform(X_train_tok)\n",
    "X_val_vectors_w2v = modelw.transform(X_test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check out my website at  and follow me on \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Check out my website at https://example.com and follow me on http://twitter.com/username!\"\n",
    "\n",
    "# Regular expression to match URLs\n",
    "url_pattern = re.compile(r'http[s]?://\\S+')\n",
    "\n",
    "# Substitute URLs with an empty string\n",
    "text_without_links = url_pattern.sub('', text)\n",
    "\n",
    "print(text_without_links)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-of-Words (with Tf-Idf) and Word Embedding with Word2Vec. You can further enhance the performance of your model using this code by\n",
    "\n",
    "using other classification algorithms like Support Vector Machines (SVM), XgBoost, Ensemble models, Neural networks etc.\n",
    "using Gridsearch to tune the hyperparameters of your model\n",
    "using advanced word-embedding methods like GloVe and BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spellchecker'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspellchecker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SpellChecker\n\u001b[0;32m      3\u001b[0m spell \u001b[38;5;241m=\u001b[39m SpellChecker()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcorrect_spellings\u001b[39m(text):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spellchecker'"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "def correct_spellings(text):\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_text.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)\n",
    "        \n",
    "text = \"corect me plese\"\n",
    "correct_spellings(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Globe, Word2Vec , pycaret , "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "example=\"New competition launched :https://www.kaggle.com/c/nlp-getting-started   in our srouse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'New competition launched :   in our srouse'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "remove_URL(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision torchaudio\n",
    "pip install tqdm  # for progress bars\n",
    "pip install scikit-learn  # for metrics like accuracy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Example dataset\n",
    "texts = [\"I love machine learning\", \"Machine learning is fascinating\", \"I love coding in Python\", ...]\n",
    "labels = [\"positive\", \"positive\", \"neutral\", ...]\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the text data (basic tokenization, use more advanced tokenizers for better performance)\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = set(token for text in X_train for token in tokenize(text))\n",
    "vocab = {word: i+1 for i, word in enumerate(vocab)}  # index 0 is reserved for padding\n",
    "\n",
    "# Convert text to sequence of indices\n",
    "def text_to_sequence(text, vocab):\n",
    "    return [vocab.get(token, 0) for token in tokenize(text)]\n",
    "\n",
    "X_train_seq = [text_to_sequence(text, vocab) for text in X_train]\n",
    "X_test_seq = [text_to_sequence(text, vocab) for text in X_test]\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, max_len=50):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Pad sequences\n",
    "        if len(text) < self.max_len:\n",
    "            text = text + [0] * (self.max_len - len(text))\n",
    "        else:\n",
    "            text = text[:self.max_len]\n",
    "        \n",
    "        return torch.tensor(text, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        pooled = torch.mean(embedded, dim=1)\n",
    "        return self.fc(pooled)\n",
    "\n",
    "# Parameters\n",
    "vocab_size = len(vocab) + 1  # +1 for padding token\n",
    "embed_dim = 128\n",
    "num_class = len(label_encoder.classes_)\n",
    "\n",
    "# Model instance\n",
    "model = TextClassificationModel(vocab_size, embed_dim, num_class)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# DataLoader\n",
    "train_dataset = TextDataset(X_train_seq, y_train)\n",
    "test_dataset = TextDataset(X_test_seq, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for texts, labels in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(texts)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}')\n",
    "\n",
    "\n",
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for texts, labels in test_loader:\n",
    "        outputs = model(texts)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_pred.extend(predicted.tolist())\n",
    "        y_true.extend(labels.tolist())\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "def predict(text, model, vocab, max_len=50):\n",
    "    model.eval()\n",
    "    text_seq = text_to_sequence(text, vocab)\n",
    "    if len(text_seq) < max_len:\n",
    "        text_seq = text_seq + [0] * (max_len - len(text_seq))\n",
    "    else:\n",
    "        text_seq = text_seq[:max_len]\n",
    "    text_tensor = torch.tensor([text_seq], dtype=torch.long)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(text_tensor)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "    \n",
    "    return label_encoder.inverse_transform(predicted.tolist())[0]\n",
    "\n",
    "# Example prediction\n",
    "print(predict(\"I enjoy learning new things\", model, vocab))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers\n",
    "pip install datasets\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('csv', data_files={'train': 'Corona_NLP_train.csv', 'test': 'Corona_NLP_test.csv'}, encoding = \"ISO-8859-1\")\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "def transform_labels(label):\n",
    "\n",
    "    label = label['Sentiment']\n",
    "    num = 0\n",
    "    if label == 'Positive':\n",
    "        num = 0\n",
    "    elif label == 'Negative':\n",
    "        num = 1\n",
    "    elif label == 'Neutral':\n",
    "        num = 2\n",
    "    elif label == 'Extremely Positive':\n",
    "        num = 3\n",
    "    elif label == 'Extremely Negative':\n",
    "        num = 4\n",
    "\n",
    "    return {'labels': num}\n",
    "\n",
    "def tokenize_data(example):\n",
    "    return tokenizer(example['OriginalTweet'], padding='max_length')\n",
    "\n",
    "dataset = dataset.map(tokenize_data, batched=True)\n",
    "\n",
    "remove_columns = ['UserName', 'ScreenName', 'Location', 'TweetAt', 'OriginalTweet', 'Sentiment']\n",
    "dataset = dataset.map(transform_labels, remove_columns=remove_columns)\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\"test_trainer\", num_train_epochs=3)\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)\n",
    "\n",
    "train_dataset = dataset['train'].shuffle(seed=10).select(range(40000))\n",
    "eval_dataset = dataset['train'].shuffle(seed=10).select(range(40000, 41000))\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "trainer.evaluate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "def load_imdb_data(data_file):\n",
    "    df = pd.read_csv(data_file)\n",
    "    texts = df['review'].tolist()\n",
    "    labels = [1 if sentiment == \"positive\" else 0 for sentiment in df['sentiment'].tolist()]\n",
    "    return texts, labels\n",
    "\n",
    "data_file = \"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\"\n",
    "texts, labels = load_imdb_data(data_file)\n",
    "\n",
    "class TextClassificationDataset(Dataset):\n",
    "\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
    "        return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'label': torch.tensor(label)}\n",
    "    \n",
    "\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_name, num_classes):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        x = self.dropout(pooled_output)\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "    \n",
    "def train(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            actual_labels.extend(labels.cpu().tolist())\n",
    "    return accuracy_score(actual_labels, predictions), classification_report(actual_labels, predictions)\n",
    "\n",
    "def predict_sentiment(text, model, tokenizer, device, max_length=128):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(text, return_tensors='pt', max_length=max_length, padding='max_length', truncation=True)\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "    return \"positive\" if preds.item() == 1 else \"negative\"\n",
    "\n",
    "# Set up parameters\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "num_classes = 2\n",
    "max_length = 128\n",
    "batch_size = 16\n",
    "num_epochs = 4\n",
    "learning_rate = 2e-5\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "train_dataset = TextClassificationDataset(train_texts, train_labels, tokenizer, max_length)\n",
    "val_dataset = TextClassificationDataset(val_texts, val_labels, tokenizer, max_length)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = BERTClassifier(bert_model_name, num_classes).to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_steps = len(train_dataloader) * num_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "\n",
    "   for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        train(model, train_dataloader, optimizer, scheduler, device)\n",
    "        accuracy, report = evaluate(model, val_dataloader, device)\n",
    "        print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "        print(report)\n",
    "\n",
    "torch.save(model.state_dict(), \"bert_classifier.pth\")\n",
    "\n",
    "\n",
    "  # Test sentiment prediction\n",
    "    test_text = \"The movie was great and I really enjoyed the performances of the actors.\"\n",
    "    sentiment = predict_sentiment(test_text, model, tokenizer, device)\n",
    "    print(\"The movie was great and I really enjoyed the performances of the actors.\")\n",
    "    print(f\"Predicted sentiment: {sentiment}\")\n",
    "\n",
    "\n",
    "# Test sentiment prediction\n",
    "    test_text = \"Worst movie of the year.\"\n",
    "    sentiment = predict_sentiment(test_text, model, tokenizer, device)\n",
    "    print(\"Worst movie of the year.\")\n",
    "    print(f\"Predicted sentiment: {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp312-cp312-win_amd64.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\abdullah\\projects\\online test\\mpoweronlinetest\\venv\\lib\\site-packages (from gensim) (1.26.2)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.13.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\abdullah\\projects\\online test\\mpoweronlinetest\\venv\\lib\\site-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: wrapt in c:\\users\\abdullah\\projects\\online test\\mpoweronlinetest\\venv\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Downloading gensim-4.3.3-cp312-cp312-win_amd64.whl (24.0 MB)\n",
      "   ---------------------------------------- 0.0/24.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.0/24.0 MB 4.6 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 1.3/24.0 MB 4.8 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 2.1/24.0 MB 3.0 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 2.9/24.0 MB 3.6 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 3.7/24.0 MB 3.5 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 4.2/24.0 MB 3.5 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 5.0/24.0 MB 3.5 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 5.8/24.0 MB 3.4 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 6.3/24.0 MB 3.3 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 6.8/24.0 MB 3.3 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 7.3/24.0 MB 3.2 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 7.9/24.0 MB 3.3 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 8.7/24.0 MB 3.2 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 9.2/24.0 MB 3.2 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 9.7/24.0 MB 3.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 10.5/24.0 MB 3.2 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 11.3/24.0 MB 3.2 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 11.8/24.0 MB 3.2 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 12.6/24.0 MB 3.2 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 13.1/24.0 MB 3.2 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 13.6/24.0 MB 3.2 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 14.2/24.0 MB 3.1 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 14.7/24.0 MB 3.1 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 15.2/24.0 MB 3.1 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 15.7/24.0 MB 3.1 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 16.3/24.0 MB 3.0 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 16.8/24.0 MB 3.0 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 17.3/24.0 MB 3.0 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 17.8/24.0 MB 3.0 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 18.4/24.0 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 18.9/24.0 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 19.1/24.0 MB 3.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 19.7/24.0 MB 2.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 19.9/24.0 MB 2.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.2/24.0 MB 2.8 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 20.7/24.0 MB 2.8 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.0/24.0 MB 2.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.2/24.0 MB 2.8 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 21.8/24.0 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.0/24.0 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.3/24.0 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.5/24.0 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.8/24.0 MB 2.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.1/24.0 MB 2.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.3/24.0 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  23.6/24.0 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  23.9/24.0 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.0/24.0 MB 2.5 MB/s eta 0:00:00\n",
      "Downloading scipy-1.13.1-cp312-cp312-win_amd64.whl (45.9 MB)\n",
      "   ---------------------------------------- 0.0/45.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/45.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/45.9 MB 1.5 MB/s eta 0:00:30\n",
      "    --------------------------------------- 0.8/45.9 MB 1.5 MB/s eta 0:00:31\n",
      "    --------------------------------------- 1.0/45.9 MB 1.5 MB/s eta 0:00:30\n",
      "    --------------------------------------- 1.0/45.9 MB 1.5 MB/s eta 0:00:30\n",
      "   - -------------------------------------- 1.6/45.9 MB 1.3 MB/s eta 0:00:34\n",
      "   - -------------------------------------- 1.6/45.9 MB 1.3 MB/s eta 0:00:34\n",
      "   - -------------------------------------- 1.8/45.9 MB 1.3 MB/s eta 0:00:35\n",
      "   - -------------------------------------- 2.1/45.9 MB 1.3 MB/s eta 0:00:36\n",
      "   -- ------------------------------------- 2.4/45.9 MB 1.3 MB/s eta 0:00:35\n",
      "   -- ------------------------------------- 2.6/45.9 MB 1.3 MB/s eta 0:00:35\n",
      "   -- ------------------------------------- 3.1/45.9 MB 1.3 MB/s eta 0:00:34\n",
      "   -- ------------------------------------- 3.4/45.9 MB 1.3 MB/s eta 0:00:34\n",
      "   --- ------------------------------------ 3.7/45.9 MB 1.3 MB/s eta 0:00:33\n",
      "   --- ------------------------------------ 3.9/45.9 MB 1.3 MB/s eta 0:00:33\n",
      "   --- ------------------------------------ 4.2/45.9 MB 1.3 MB/s eta 0:00:32\n",
      "   --- ------------------------------------ 4.5/45.9 MB 1.3 MB/s eta 0:00:32\n",
      "   ---- ----------------------------------- 4.7/45.9 MB 1.3 MB/s eta 0:00:32\n",
      "   ---- ----------------------------------- 5.0/45.9 MB 1.3 MB/s eta 0:00:31\n",
      "   ---- ----------------------------------- 5.2/45.9 MB 1.3 MB/s eta 0:00:31\n",
      "   ---- ----------------------------------- 5.5/45.9 MB 1.3 MB/s eta 0:00:31\n",
      "   ----- ---------------------------------- 6.0/45.9 MB 1.4 MB/s eta 0:00:30\n",
      "   ----- ---------------------------------- 6.3/45.9 MB 1.4 MB/s eta 0:00:30\n",
      "   ----- ---------------------------------- 6.6/45.9 MB 1.4 MB/s eta 0:00:29\n",
      "   ----- ---------------------------------- 6.8/45.9 MB 1.4 MB/s eta 0:00:29\n",
      "   ------ --------------------------------- 7.3/45.9 MB 1.4 MB/s eta 0:00:28\n",
      "   ------ --------------------------------- 7.6/45.9 MB 1.4 MB/s eta 0:00:28\n",
      "   ------ --------------------------------- 7.9/45.9 MB 1.4 MB/s eta 0:00:28\n",
      "   ------- -------------------------------- 8.1/45.9 MB 1.4 MB/s eta 0:00:28\n",
      "   ------- -------------------------------- 8.4/45.9 MB 1.4 MB/s eta 0:00:28\n",
      "   ------- -------------------------------- 8.7/45.9 MB 1.4 MB/s eta 0:00:28\n",
      "   ------- -------------------------------- 8.9/45.9 MB 1.4 MB/s eta 0:00:27\n",
      "   ------- -------------------------------- 9.2/45.9 MB 1.4 MB/s eta 0:00:27\n",
      "   -------- ------------------------------- 9.4/45.9 MB 1.4 MB/s eta 0:00:27\n",
      "   -------- ------------------------------- 9.7/45.9 MB 1.4 MB/s eta 0:00:27\n",
      "   -------- ------------------------------- 10.2/45.9 MB 1.4 MB/s eta 0:00:26\n",
      "   --------- ------------------------------ 10.5/45.9 MB 1.4 MB/s eta 0:00:26\n",
      "   --------- ------------------------------ 10.7/45.9 MB 1.4 MB/s eta 0:00:26\n",
      "   --------- ------------------------------ 11.0/45.9 MB 1.4 MB/s eta 0:00:25\n",
      "   ---------- ----------------------------- 11.5/45.9 MB 1.4 MB/s eta 0:00:25\n",
      "   ---------- ----------------------------- 11.8/45.9 MB 1.4 MB/s eta 0:00:25\n",
      "   ---------- ----------------------------- 12.1/45.9 MB 1.4 MB/s eta 0:00:24\n",
      "   ---------- ----------------------------- 12.3/45.9 MB 1.4 MB/s eta 0:00:24\n",
      "   ----------- ---------------------------- 12.8/45.9 MB 1.4 MB/s eta 0:00:24\n",
      "   ----------- ---------------------------- 13.1/45.9 MB 1.4 MB/s eta 0:00:23\n",
      "   ----------- ---------------------------- 13.4/45.9 MB 1.4 MB/s eta 0:00:23\n",
      "   ----------- ---------------------------- 13.6/45.9 MB 1.4 MB/s eta 0:00:23\n",
      "   ------------ --------------------------- 13.9/45.9 MB 1.5 MB/s eta 0:00:23\n",
      "   ------------ --------------------------- 14.2/45.9 MB 1.4 MB/s eta 0:00:23\n",
      "   ------------ --------------------------- 14.4/45.9 MB 1.4 MB/s eta 0:00:23\n",
      "   ------------ --------------------------- 14.7/45.9 MB 1.4 MB/s eta 0:00:22\n",
      "   ------------- -------------------------- 14.9/45.9 MB 1.4 MB/s eta 0:00:22\n",
      "   ------------- -------------------------- 15.2/45.9 MB 1.4 MB/s eta 0:00:22\n",
      "   ------------- -------------------------- 15.5/45.9 MB 1.4 MB/s eta 0:00:22\n",
      "   ------------- -------------------------- 16.0/45.9 MB 1.4 MB/s eta 0:00:21\n",
      "   -------------- ------------------------- 16.3/45.9 MB 1.4 MB/s eta 0:00:21\n",
      "   -------------- ------------------------- 16.5/45.9 MB 1.4 MB/s eta 0:00:21\n",
      "   -------------- ------------------------- 17.0/45.9 MB 1.4 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 17.0/45.9 MB 1.4 MB/s eta 0:00:20\n",
      "   --------------- ------------------------ 17.3/45.9 MB 1.4 MB/s eta 0:00:20\n",
      "   --------------- ------------------------ 17.6/45.9 MB 1.4 MB/s eta 0:00:20\n",
      "   --------------- ------------------------ 17.8/45.9 MB 1.4 MB/s eta 0:00:20\n",
      "   --------------- ------------------------ 18.1/45.9 MB 1.4 MB/s eta 0:00:20\n",
      "   --------------- ------------------------ 18.4/45.9 MB 1.4 MB/s eta 0:00:20\n",
      "   ---------------- ----------------------- 18.6/45.9 MB 1.4 MB/s eta 0:00:20\n",
      "   ---------------- ----------------------- 18.9/45.9 MB 1.4 MB/s eta 0:00:20\n",
      "   ---------------- ----------------------- 19.1/45.9 MB 1.4 MB/s eta 0:00:19\n",
      "   ---------------- ----------------------- 19.4/45.9 MB 1.4 MB/s eta 0:00:19\n",
      "   ----------------- ---------------------- 19.7/45.9 MB 1.4 MB/s eta 0:00:19\n",
      "   ----------------- ---------------------- 19.9/45.9 MB 1.4 MB/s eta 0:00:19\n",
      "   ----------------- ---------------------- 20.2/45.9 MB 1.4 MB/s eta 0:00:19\n",
      "   ----------------- ---------------------- 20.4/45.9 MB 1.4 MB/s eta 0:00:19\n",
      "   ------------------ --------------------- 20.7/45.9 MB 1.4 MB/s eta 0:00:19\n",
      "   ------------------ --------------------- 21.0/45.9 MB 1.4 MB/s eta 0:00:18\n",
      "   ------------------ --------------------- 21.2/45.9 MB 1.4 MB/s eta 0:00:18\n",
      "   ------------------ --------------------- 21.5/45.9 MB 1.4 MB/s eta 0:00:18\n",
      "   ------------------ --------------------- 21.8/45.9 MB 1.4 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 22.0/45.9 MB 1.4 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 22.0/45.9 MB 1.4 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 22.5/45.9 MB 1.4 MB/s eta 0:00:17\n",
      "   ------------------- -------------------- 22.5/45.9 MB 1.4 MB/s eta 0:00:17\n",
      "   ------------------- -------------------- 22.8/45.9 MB 1.4 MB/s eta 0:00:17\n",
      "   -------------------- ------------------- 23.1/45.9 MB 1.4 MB/s eta 0:00:17\n",
      "   -------------------- ------------------- 23.3/45.9 MB 1.4 MB/s eta 0:00:17\n",
      "   -------------------- ------------------- 23.6/45.9 MB 1.4 MB/s eta 0:00:17\n",
      "   -------------------- ------------------- 23.9/45.9 MB 1.4 MB/s eta 0:00:17\n",
      "   --------------------- ------------------ 24.1/45.9 MB 1.4 MB/s eta 0:00:17\n",
      "   --------------------- ------------------ 24.1/45.9 MB 1.4 MB/s eta 0:00:17\n",
      "   --------------------- ------------------ 24.4/45.9 MB 1.4 MB/s eta 0:00:16\n",
      "   --------------------- ------------------ 24.6/45.9 MB 1.4 MB/s eta 0:00:16\n",
      "   --------------------- ------------------ 24.9/45.9 MB 1.4 MB/s eta 0:00:16\n",
      "   --------------------- ------------------ 25.2/45.9 MB 1.4 MB/s eta 0:00:16\n",
      "   ---------------------- ----------------- 25.4/45.9 MB 1.4 MB/s eta 0:00:16\n",
      "   ---------------------- ----------------- 25.7/45.9 MB 1.3 MB/s eta 0:00:16\n",
      "   ---------------------- ----------------- 25.7/45.9 MB 1.3 MB/s eta 0:00:16\n",
      "   ---------------------- ----------------- 26.0/45.9 MB 1.3 MB/s eta 0:00:15\n",
      "   ---------------------- ----------------- 26.2/45.9 MB 1.3 MB/s eta 0:00:15\n",
      "   ----------------------- ---------------- 26.5/45.9 MB 1.3 MB/s eta 0:00:15\n",
      "   ----------------------- ---------------- 26.5/45.9 MB 1.3 MB/s eta 0:00:15\n",
      "   ----------------------- ---------------- 26.7/45.9 MB 1.3 MB/s eta 0:00:15\n",
      "   ----------------------- ---------------- 26.7/45.9 MB 1.3 MB/s eta 0:00:15\n",
      "   ----------------------- ---------------- 27.0/45.9 MB 1.3 MB/s eta 0:00:15\n",
      "   ----------------------- ---------------- 27.0/45.9 MB 1.3 MB/s eta 0:00:15\n",
      "   ----------------------- ---------------- 27.3/45.9 MB 1.3 MB/s eta 0:00:15\n",
      "   ----------------------- ---------------- 27.3/45.9 MB 1.3 MB/s eta 0:00:15\n",
      "   ----------------------- ---------------- 27.5/45.9 MB 1.3 MB/s eta 0:00:15\n",
      "   ------------------------ --------------- 27.8/45.9 MB 1.3 MB/s eta 0:00:15\n",
      "   ------------------------ --------------- 27.8/45.9 MB 1.3 MB/s eta 0:00:15\n",
      "   ------------------------ --------------- 28.0/45.9 MB 1.3 MB/s eta 0:00:15\n",
      "   ------------------------ --------------- 28.3/45.9 MB 1.3 MB/s eta 0:00:14\n",
      "   ------------------------ --------------- 28.3/45.9 MB 1.3 MB/s eta 0:00:14\n",
      "   ------------------------ --------------- 28.6/45.9 MB 1.3 MB/s eta 0:00:14\n",
      "   ------------------------ --------------- 28.6/45.9 MB 1.3 MB/s eta 0:00:14\n",
      "   ------------------------- -------------- 28.8/45.9 MB 1.2 MB/s eta 0:00:14\n",
      "   ------------------------- -------------- 28.8/45.9 MB 1.2 MB/s eta 0:00:14\n",
      "   ------------------------- -------------- 29.1/45.9 MB 1.2 MB/s eta 0:00:14\n",
      "   ------------------------- -------------- 29.4/45.9 MB 1.2 MB/s eta 0:00:14\n",
      "   ------------------------- -------------- 29.4/45.9 MB 1.2 MB/s eta 0:00:14\n",
      "   ------------------------- -------------- 29.6/45.9 MB 1.2 MB/s eta 0:00:14\n",
      "   -------------------------- ------------- 29.9/45.9 MB 1.2 MB/s eta 0:00:14\n",
      "   -------------------------- ------------- 30.1/45.9 MB 1.2 MB/s eta 0:00:13\n",
      "   -------------------------- ------------- 30.1/45.9 MB 1.2 MB/s eta 0:00:13\n",
      "   -------------------------- ------------- 30.4/45.9 MB 1.2 MB/s eta 0:00:13\n",
      "   -------------------------- ------------- 30.7/45.9 MB 1.2 MB/s eta 0:00:13\n",
      "   -------------------------- ------------- 30.7/45.9 MB 1.2 MB/s eta 0:00:13\n",
      "   -------------------------- ------------- 30.9/45.9 MB 1.2 MB/s eta 0:00:13\n",
      "   --------------------------- ------------ 31.2/45.9 MB 1.2 MB/s eta 0:00:13\n",
      "   --------------------------- ------------ 31.2/45.9 MB 1.2 MB/s eta 0:00:13\n",
      "   --------------------------- ------------ 31.5/45.9 MB 1.2 MB/s eta 0:00:13\n",
      "   --------------------------- ------------ 31.7/45.9 MB 1.2 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 32.0/45.9 MB 1.2 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 32.0/45.9 MB 1.2 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 32.2/45.9 MB 1.2 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 32.5/45.9 MB 1.2 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 32.8/45.9 MB 1.2 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 32.8/45.9 MB 1.2 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 33.0/45.9 MB 1.2 MB/s eta 0:00:11\n",
      "   ----------------------------- ---------- 33.3/45.9 MB 1.2 MB/s eta 0:00:11\n",
      "   ----------------------------- ---------- 33.6/45.9 MB 1.2 MB/s eta 0:00:11\n",
      "   ----------------------------- ---------- 33.8/45.9 MB 1.2 MB/s eta 0:00:11\n",
      "   ----------------------------- ---------- 34.1/45.9 MB 1.2 MB/s eta 0:00:10\n",
      "   ----------------------------- ---------- 34.3/45.9 MB 1.2 MB/s eta 0:00:10\n",
      "   ----------------------------- ---------- 34.3/45.9 MB 1.2 MB/s eta 0:00:10\n",
      "   ------------------------------ --------- 34.6/45.9 MB 1.2 MB/s eta 0:00:10\n",
      "   ------------------------------ --------- 34.6/45.9 MB 1.2 MB/s eta 0:00:10\n",
      "   ------------------------------ --------- 34.9/45.9 MB 1.2 MB/s eta 0:00:10\n",
      "   ------------------------------ --------- 35.1/45.9 MB 1.2 MB/s eta 0:00:10\n",
      "   ------------------------------ --------- 35.1/45.9 MB 1.2 MB/s eta 0:00:10\n",
      "   ------------------------------ --------- 35.4/45.9 MB 1.2 MB/s eta 0:00:10\n",
      "   ------------------------------ --------- 35.4/45.9 MB 1.2 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 35.7/45.9 MB 1.2 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 35.9/45.9 MB 1.2 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 35.9/45.9 MB 1.2 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 36.2/45.9 MB 1.2 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 36.4/45.9 MB 1.2 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 36.7/45.9 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 36.7/45.9 MB 1.1 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 37.0/45.9 MB 1.1 MB/s eta 0:00:08\n",
      "   -------------------------------- ------- 37.2/45.9 MB 1.1 MB/s eta 0:00:08\n",
      "   -------------------------------- ------- 37.5/45.9 MB 1.1 MB/s eta 0:00:08\n",
      "   -------------------------------- ------- 37.7/45.9 MB 1.1 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 38.0/45.9 MB 1.1 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 38.3/45.9 MB 1.1 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 38.5/45.9 MB 1.1 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 38.8/45.9 MB 1.1 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 39.1/45.9 MB 1.1 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 39.3/45.9 MB 1.1 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 39.6/45.9 MB 1.1 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 39.8/45.9 MB 1.1 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 40.1/45.9 MB 1.1 MB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 40.4/45.9 MB 1.1 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 40.6/45.9 MB 1.1 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 40.9/45.9 MB 1.1 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 41.4/45.9 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 41.7/45.9 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 41.9/45.9 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 42.2/45.9 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 42.7/45.9 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 43.0/45.9 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 43.3/45.9 MB 1.1 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 43.8/45.9 MB 1.1 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 44.0/45.9 MB 1.1 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 44.6/45.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------------------------  44.8/45.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  45.4/45.9 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  45.6/45.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 45.9/45.9 MB 1.2 MB/s eta 0:00:00\n",
      "Installing collected packages: scipy, gensim\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.14.0\n",
      "    Uninstalling scipy-1.14.0:\n",
      "      Successfully uninstalled scipy-1.14.0\n",
      "Successfully installed gensim-4.3.3 scipy-1.13.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abdullah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['natural',\n",
       "  'language',\n",
       "  'processing',\n",
       "  'makes',\n",
       "  'computers',\n",
       "  'understand',\n",
       "  'text',\n",
       "  'speech'],\n",
       " ['machine', 'learning', 'subset', 'artificial', 'intelligence'],\n",
       " ['word', 'embeddings', 'capture', 'semantic', 'meanings', 'words'],\n",
       " ['deep', 'learning', 'models', 'complex', 'require', 'lot', 'data']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"Natural language processing makes computers understand text and speech.\",\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"Word embeddings capture semantic meanings of words.\",\n",
    "    \"Deep learning models are complex and require a lot of data.\"\n",
    "]\n",
    "\n",
    "# Preprocess the sentences (tokenize and remove stopwords)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "processed_sentences = [[word for word in simple_preprocess(sentence) if word not in stop_words] for sentence in sentences]\n",
    "processed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 260)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set parameters for Word2Vec\n",
    "model = Word2Vec(\n",
    "    sentences=processed_sentences,\n",
    "    vector_size=100,  # Dimensionality of the word vectors\n",
    "    window=5,         # Context window size\n",
    "    min_count=1,      # Ignores words with total frequency lower than this\n",
    "    workers=4,        # Number of worker threads to train the model\n",
    "    sg=0              # Training algorithm: 1 for skip-gram; 0 for CBOW\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.train(processed_sentences, total_examples=len(processed_sentences), epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8.2428791e-03  9.2996312e-03 -2.0324520e-04 -1.9623127e-03\n",
      "  4.6078889e-03 -4.0994231e-03  2.7446921e-03  6.9502438e-03\n",
      "  6.0624750e-03 -7.5168125e-03  9.3859164e-03  4.6658926e-03\n",
      "  3.9652535e-03 -6.2409118e-03  8.4593408e-03 -2.1446843e-03\n",
      "  8.8366689e-03 -5.3645796e-03 -8.1226956e-03  6.8163285e-03\n",
      "  1.6780979e-03 -2.1975953e-03  9.5146457e-03  9.4916159e-03\n",
      " -9.7695924e-03  2.5073413e-03  6.1499067e-03  3.8820067e-03\n",
      "  2.0254797e-03  4.2475312e-04  6.8322109e-04 -3.8205353e-03\n",
      " -7.1395137e-03 -2.0916054e-03  3.9166659e-03  8.8147186e-03\n",
      "  9.2658531e-03 -5.9704902e-03 -9.4034951e-03  9.7668255e-03\n",
      "  3.4359887e-03  5.1680701e-03  6.2840963e-03 -2.8036023e-03\n",
      "  7.3251370e-03  2.8331103e-03  2.8640917e-03 -2.3796128e-03\n",
      " -3.1250229e-03 -2.3669479e-03  4.2813178e-03  8.1156031e-05\n",
      " -9.5881447e-03 -9.6666887e-03 -6.1505702e-03 -1.3877029e-04\n",
      "  1.9965856e-03  9.4291028e-03  5.5879811e-03 -4.2947554e-03\n",
      "  2.7267737e-04  4.9670311e-03  7.7122939e-03 -1.1377098e-03\n",
      "  4.3169018e-03 -5.8082542e-03 -8.0188492e-04  8.0997506e-03\n",
      " -2.3654606e-03 -9.6619884e-03  5.7797246e-03 -3.9246827e-03\n",
      " -1.2220819e-03  9.9757323e-03 -2.2517033e-03 -4.7646910e-03\n",
      " -5.3224647e-03  6.9860942e-03 -5.7072146e-03  2.1129833e-03\n",
      " -5.2632992e-03  6.1138934e-03  4.3594567e-03  2.6109128e-03\n",
      " -1.4896488e-03 -2.7521984e-03  8.9973519e-03  5.2173608e-03\n",
      " -2.1656156e-03 -9.4717117e-03 -7.4258717e-03 -1.0611786e-03\n",
      " -7.8680023e-04 -2.5567575e-03  9.6943434e-03 -4.6442263e-04\n",
      "  5.8745528e-03 -7.4396911e-03 -2.5008209e-03 -5.5402862e-03]\n",
      "[('word', 0.17863720655441284), ('makes', 0.13139469921588898), ('text', 0.07509029656648636), ('data', 0.06804186850786209), ('natural', 0.04813927039504051), ('computers', 0.041638027876615524), ('subset', 0.04144083708524704), ('semantic', 0.04117994010448456), ('complex', 0.040998633950948715), ('deep', 0.03130533546209335)]\n"
     ]
    }
   ],
   "source": [
    "# Get the vector for a word\n",
    "word_vector = model.wv['language']\n",
    "print(word_vector)\n",
    "\n",
    "# Find similar words\n",
    "similar_words = model.wv.most_similar('language')\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
